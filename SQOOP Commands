LIST DATABASES (Listing databases in MySql through sqoop commands with the help of jdbc connector)
-------------
[hduser@ubuntu ~]$ sqoop list-databases --connect jdbc:mysql://localhost --username root --password 'hduser';

LIST TABLES in a database (Listing tables in MySql through sqoop commands with the help of jdbc connector)
--------------------------
[hduser@ubuntu ~]$ sqoop list-tables --connect jdbc:mysql://localhost/college --username root --password 'hduser';

Import one table (with primary key)from mysql into HDFS
------------------------------------------------
[hduser@ubuntu ~]$ sqoop import --connect jdbc:mysql://localhost/college --username root --password 'hduser' --table student_master --target-dir /hanoi/student_master;
(Here this table(student_master) in mysql has a primary key,so here even if we dont specify the mapper count, it will create 4 mappers(default) in order of the primary key,but if this table hasnt got any primary key and also if we wont mention the mapper count then 4 mappers will be created without any order of the elements within it)

Add an extra record in mysql in college db
-------------------------------------------
INSERT INTO student_master
     (name, address)
     VALUES
    ("New", "New Place");

WITH INCREMENTAL(It updates the already existing student_master table in hdfs with any new values that is added in the MySql table.For eg:above we could see an extra row added in student_master in mysql,so this data is updated to student_master in hdfs using INCREMENTAL )
-------------------------------------------------------------------------------------------------------------------------------------------------
sqoop import --connect jdbc:mysql://localhost/college --username root --password 'hduser' --table student_master --check-column student_id --incremental append --last-value  8 --target-dir /niit/student_master;
(Here "--last-value  8" represents the last row present in student_master before appending the new value to the table in hdfs,so when a new value is added to table in hdfs it would become the 9th row)

COMPRESS FILE (It creates a Zip file of the imported MySql data in hdfs)
-------------
sqoop import --connect jdbc:mysql://localhost/college --username root --password 'hduser' --table student_master --target-dir /niit/student_master1 --compress -m 1;

Import one table (without key)from mysql into HDFS
------------------------------------------------
[hduser@ubuntu ~]$ sqoop import --connect jdbc:mysql://localhost/college --username root --password 'hduser' --table topten --target-dir /niit/topten -m 1 ;
(Here this table(topten) hasnt defined with any primary key,so we would specify the mapper count,otherwise 4 mappers without any order of elements within it would be created)

import table of avro type 
--------------------------
sqoop import --connect jdbc:mysql://localhost/college --username root --password 'hduser' --table topten --target-dir /niit/top10avro --as-avrodatafile -m 1 ; 
(Here the topten table in mysql which is a text dataset table is imported to HDFS in an avro format as an avro datafile.Then for this avro datafile in hdfs, an avro table with Serde properties is created in hive and the avro datafile from hdfs is loaded into hive,which is done below,therefore we can now read this avro datafile as normal datafile through hive)

(In HIVE)create avro table and load data in hive
----------------------------------------
CREATE TABLE toptenavro
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT
'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES (
    'avro.schema.literal'='{
      "namespace": "abc",
      "name": "top10",
      "type": "record",
      "fields": [
{ "name":"customer_id","type":"int"}, {
"name":"fname","type":"string"}, { "name":"lname","type":"string"},{
"name":"age","type":"int"},{ "name":"profession","type":"string"},{
"name":"amount","type":"double"}]
    }');

(IN HIVE)load data inpath '/niit/top10avro/part-m-00000.avro' overwrite into
table toptenavro;


import table of a sequence type
-------------------------------
sqoop import --connect jdbc:mysql://localhost/college --username root --password 'hduser' --table topten --target-dir /niit/top10seq --as-sequencefile -m 1 ;

WITH WHERE CLAUSE
------------------
sqoop import --connect jdbc:mysql://localhost/college --username root --password 'hduser' --table student_master --where 'student_id=1 or student_id=3' --target-dir /niit/query -m 1;

WITH QUERY
-----------
sqoop import --connect jdbc:mysql://localhost/college --username root --password 'hduser' --query 'select * from student_master where $CONDITIONS and student_id=2' --target-dir /niit/query1 -m 1;
(Here when query is used within sqoop commands then "$CONDITIONS and" is necessary,otherwise it would throw error on executing this sqoop commands)

JOINS
-----

WITH INNER JOINS in form of Query
----------------------------------
sqoop import --connect jdbc:mysql://localhost/college --username root --password 'hduser' --query 'select a.student_id, a.name, a.address, b.result from student_master a, fy b where $CONDITIONS and a.student_id=b.student_id' --target-dir /niit/query2 -m 1;


WITH LEFT OUTER JOINS in form of Query
---------------------------------------
sqoop import --connect jdbc:mysql://localhost/college --username root --password 'hduser' --query 'select a.student_id, a.name, a.address,
b.result from student_master a left outer join fy b on a.student_id = b.student_id where $CONDITIONS' --target-dir /niit/query3 -m 1;

WITH RIGHT OUTER JOINS in form of Query
----------------------------------------
sqoop import --connect jdbc:mysql://localhost/college --username root --password 'hduser' --query 'select a.student_id, a.name, a.address,
b.result from student_master a right outer join fy b on a.student_id = b.student_id where $CONDITIONS' --target-dir /niit/RightOuterJoin -m 1;

WITH COLUMN CLAUSE (Here we can extract the data column wise in a table in mysql and import it to hdfs)
---------------------------------------------------------------------------------------------------------
sqoop import --connect jdbc:mysql://localhost/college --username root --password 'hduser' --table student_master --columns "student_id,name"
--target-dir /niit/query5 -m 1;

Import ALL TABLES from mysql into hdfs (Here we can import all the tables in mysql within one database in one go without individually importing to HDFS)(Here another thing to be noted is, we should always mention mapper count when importing all tables in one go because if any of the tables is not having a primary key,then error can occur,so using mapper count will enable proper import)
------------------------------------------------------------------------------------------------------------------------------------------------
[hduser@ubuntu ~]$ sqoop import-all-tables --connect jdbc:mysql://localhost/college --username root --password 'hduser' --warehouse-dir /niit/all_tables -m 1;

Import data into hive managed tables
-------------------------------------
First create a database college in hive
hive> create database college; (so here a folder named college.db will be created in '/user/hive/warehouse' on hdfs)
hive> quit;

sqoop import --connect jdbc:mysql://localhost/college --username  root --password 'hduser' --table student_master --hive-import --hive-table
college.student_profile -m 1;
(Here the table in mysql is imported to hive managed tables i.e to hdfs only since hive tables str is only in lfs, the data is always stored in hdfs only, so 'student_profile' named folder is created under 'college.db' on hdfs when using '--hive-table college.student_profile' in sqoop command)

sqoop import --connect jdbc:mysql://localhost/college --username  root --password 'hduser' --table fy --hive-import --hive-table college.fyresults -m 1;

Columnwise extraction of data from table in sql to hive
-------------------------------------------------------
sqoop import --connect jdbc:mysql://localhost/college --username  root --password 'hduser' --table student_master --columns "student_id,name"
--hive-import --hive-table college.student1 -m 1;

Sqoop imports using where clause
---------------------------------
sqoop import --connect jdbc:mysql://localhost/college --username  root --password 'hduser' --table student_master --where "student_id = 1 or student_id=3" --hive-import --hive-table college.student2 -m 1;

Sqoop imports with query(Here "$CONDITIONS and" is necessary in query using sqoop imports because without it error will be thrown)
-----------------------------------------------------------------------------------------------------------------------------
sqoop import --connect jdbc:mysql://localhost/college --username root --password 'hduser' --query 'select a.student_id, a.name, a.address, b.result from student_master a, fy b where $CONDITIONS and a.student_id=b.student_id' --hive-import --hive-table college.studentjoin --target-dir /niit/query20 -m 1;

Sqoop EXPORT Commands
-----------------------
nano /home/hduser/employee.txt

1201,satish,delhi
1202,krishna,mumbai
1203,amith,pune
1204,javed,chennai
1205,prudvi,bangalore

hadoop fs -put employee.txt /niit

nano /home/hduser/emp1.txt

1201,satish1,delhi
1202,krishna1,mumbai
1206,sanjay,pune
1207,rajiv,chennai
1208,vijay,bangalore

hadoop fs -put emp1.txt /niit

In MySQl
use college;

CREATE TABLE employee_master(
   employee_id INT NOT NULL AUTO_INCREMENT,
   name VARCHAR(40) NOT NULL,
   address VARCHAR(40) NOT NULL,
   PRIMARY KEY ( employee_id ));


sqoop export --connect jdbc:mysql://localhost/college --username root --password 'hduser' --table employee_master --update-mode  allowinsert --update-key employee_id   --export-dir /niit/employee.txt --input-fields-terminated-by ',';
(Here 'college' is the database created within MySql in which the table named employee_master is created to export the data from hdfs to MySql)
("--update-mode  allowinsert" allows the table to be updated preventing duplicate values within the table)
(" --update-key" allows the updation of the primary key of the table)
(" --export-dir" is the directory/folder in hdfs, from where the data is exported to MySql)

Project sqoop command[sqoop export --connect jdbc:mysql://localhost/project --username root --password 'hduser' --table h1bfinal1 --update-mode allowinsert --update-key job_title --export-dir /ProjectOutputs/Query10Op/p* --input-fields-terminated-by '\t']


sqoop export --connect jdbc:mysql://localhost/college --username root --password 'hduser' --table employee_master --update-mode  allowinsert --update-key employee_id --export-dir /niit/emp1.txt --input-fields-terminated-by ',' ;
(Here emp1.txt file is also exported to MySql(to employee_master table) from hdfs. This file contains updated values of the already existing values in employee.txt,so the old values of employee.txt gets overriden by this new values in emp.txt with the help of "--update-mode  allowinsert", thus avoiding duplicate values within the employee_master table)

CREATE TABLE customer(
   customer_id VARCHAR(20) NOT NULL,
   firstname VARCHAR(40) NOT NULL,
      lastname VARCHAR(40) NOT NULL,
   age int NOT NULL,
profession VARCHAR(40) NOT NULL,
   PRIMARY KEY ( customer_id ));


sqoop export --connect jdbc:mysql://localhost/college --username root --password 'hduser' --table customer --update-mode  allowinsert --update-key customer_id   --export-dir /Part --input-fields-terminated-by ',' ;
(Here "/Part" is the location in hdfs where the dataset of customer table which is an external table in hive is located.Therefore we can also export an external table in hive to MySql, through Sqoop)
